{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yeleao/NovelAI/blob/main/NovelAILeaks_API_Backend_(4chan_Ver_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "基于 4chan 魔改版 NovelAILeaks (naifu) 制作。[来源](https://boards.4channel.org/g/thread/89095460#p89097704)\n",
        "\n",
        "使用官方前端 + 优化版后端，可突破75限制，支持所有模型。\n",
        "\n",
        "Credit: https://t.me/StableDiffusion_CN https://t.me/exlolicon\n",
        "\n",
        "Thanks: Anonymous, 炼铜术士, 神楽坂早苗️, Jonathan, 咕 咕, 猫又逆变器, Gaein nidb\n",
        "\n"
      ],
      "metadata": {
        "id": "KZ88G-iWCTs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5yF8TS1CR3L"
      },
      "outputs": [],
      "source": [
        "#@title ### 0. 检查 GPU 工作状态\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OsI3VNs42vR-",
        "outputId": "b69e7415-e7a8-427b-8ee5-b256a64853f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cOo7vY78RFu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 1. 下载 Novel AI API 后端、模型 \n",
        "#@markdown 如果下载速度太慢可尝试 restart\n",
        "\n",
        "%cd /content/\n",
        "!apt install -y -qq aria2\n",
        "!aria2c --summary-interval=5 -x 3 --allow-overwrite=true -Z \\\n",
        "  https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/naifu.tar \\\n",
        "  https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar \n",
        "  \n",
        "!echo \"Decompressing...\"\n",
        "!tar xf naifu.tar && rm naifu.tar\n",
        "!echo \"Done.\""
      ],
      "metadata": {
        "id": "iqTO_Uf3F6VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 2. 安装依赖\n",
        "#@markdown 耐心等待安装完成\n",
        "\n",
        "%cd /content/naifu\n",
        "!pip install virtualenv && bash ./setup.sh\n",
        "!curl -Ls https://github.com/ekzhang/bore/releases/download/v0.4.0/bore-v0.4.0-x86_64-unknown-linux-musl.tar.gz | tar zx -C /usr/bin\n",
        "!curl -Lo /usr/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x /usr/bin/cloudflared"
      ],
      "metadata": {
        "id": "BysBfYRmGSo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 3. 启动模型\n",
        "#@markdown 访问输出的映射地址（以 `trycloudflare.com` / `bore.pub` 结尾）即可。\n",
        "#@markdown - 请等待模型加载完成（出现`Application startup complete`字样）再访问\n",
        "#@markdown - cloudflare 提供的服务偶尔会出现请求超时，可换用 bore 隧道\n",
        "\n",
        "%cd /content/naifu\n",
        "!sed -i 's/# export SAVE_FILES=\"1\"/export SAVE_FILES=\"1\"/g' run.sh\n",
        "!bash run.sh & cloudflared tunnel --url localhost:6969 "
      ],
      "metadata": {
        "id": "uQBR9zXQGJrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 4. （可选）使用 7G 的 `animefull-latest` 模型运行\n",
        "#@markdown 默认使用的是 4G 大小的 animefull-final-pruned 模型。如果想使用 7G 的 animefull-latest 模型，运行这个\n",
        "\n",
        "%cd /content/\n",
        "!tar xf animefull-latest.tar -C /content/naifu/models && rm animefull-latest.tar\n",
        "!sed -i 's/map_location=\"cpu\"/map_location=\"cuda\"/g' /content/naifu/hydra_node/models.py\n",
        "\n",
        "%cd /content/naifu\n",
        "%env DTYPE=float16\n",
        "%env CLIP_CONTEXTS=3\n",
        "%env AMP=1\n",
        "%env MODEL=stable-diffusion\n",
        "%env DEV=True\n",
        "%env MODEL_PATH=models/animefull-latest\n",
        "%env ENABLE_EMA=1\n",
        "%env VAE_PATH=models/animevae.pt\n",
        "%env PENULTIMATE=1\n",
        "%env PYTHONDONTWRITEBYTECODE=1\n",
        "%env SAVE_FILES=1\n",
        "\n",
        "!./venv/bin/python -m uvicorn --host 127.0.0.1 --port=7860 main:app & bore local 6969 --to bore.pub & cloudflared tunnel --url localhost:6969 "
      ],
      "metadata": {
        "id": "B9j9thAby5_2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0f8fcb1-8f8e-4892-8fc9-cec13195e41f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "tar: animefull-latest.tar: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "/content/naifu\n",
            "env: DTYPE=float16\n",
            "env: CLIP_CONTEXTS=3\n",
            "env: AMP=1\n",
            "env: MODEL=stable-diffusion\n",
            "env: DEV=True\n",
            "env: MODEL_PATH=models/animefull-latest\n",
            "env: ENABLE_EMA=1\n",
            "env: VAE_PATH=models/animevae.pt\n",
            "env: PENULTIMATE=1\n",
            "env: PYTHONDONTWRITEBYTECODE=1\n",
            "env: SAVE_FILES=1\n",
            "\u001b[90m2022-10-15T10:38:25Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2022-10-15T10:38:25Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[2m2022-10-15T10:38:25.272368Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m connected to server \u001b[3mremote_port\u001b[0m\u001b[2m=\u001b[0m46849\n",
            "\u001b[2m2022-10-15T10:38:25.272428Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m listening at bore.pub:46849\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m |  https://creates-communist-jokes-bookstore.trycloudflare.com                               |\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m Version 2022.10.0\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.18.6, GoArch: amd64\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[protocol:quic url:localhost:6969]\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 2d101238-a4b9-4a54-bae5-689b0ece6e22\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/run-tunnel/as-a-service/\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.2 as source for IPv4\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[31mWRN\u001b[0m The user running cloudflared process has a GID (group ID) that is not within ping_group_range. You might need to add that user to a group within that range, or instead update the range to encompass a group the user is already in by modifying /proc/sys/net/ipv4/ping_group_range. Otherwise cloudflared will not be able to ping this network \u001b[31merror=\u001b[0m\u001b[31m\"Group ID 0 is not between ping group 1 to 0\"\u001b[0m\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[31mWRN\u001b[0m ICMP proxy feature is disabled \u001b[31merror=\u001b[0m\u001b[31m\"cannot create ICMPv4 proxy: Group ID 0 is not between ping group 1 to 0 nor ICMPv6 proxy: socket: permission denied\"\u001b[0m\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:45007/metrics\n",
            "2022/10/15 10:38:26 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve quic connection \u001b[31merror=\u001b[0m\u001b[31m\"Unauthorized: Failed to get tunnel\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.193\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Register tunnel error from server side \u001b[31merror=\u001b[0m\u001b[31m\"Unauthorized: Failed to get tunnel\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.193\n",
            "\u001b[90m2022-10-15T10:38:26Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 2s seconds \u001b[36mconnIndex=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.193\n",
            "\u001b[90m2022-10-15T10:38:28Z\u001b[0m \u001b[32mINF\u001b[0m Connection d8b6b4a2-dd43-403e-a503-601756ba2e9f registered \u001b[36mconnIndex=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.193 \u001b[36mlocation=\u001b[0mORD\n",
            "Starting Hydra Node HTTP TOKEN=None\n",
            "2022-10-15 10:38:28,761 INFO config.py(2490) - CPU: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "2022-10-15 10:38:28,766 INFO config.py(2490) - GPU: Tesla T4\n",
            "2022-10-15 10:38:28,766 INFO config.py(2490) - GPU RAM: 15gb\n",
            "2022-10-15 10:38:28,766 INFO config.py(2490) - MODEL: stable-diffusion\n",
            "2022-10-15 10:38:28,766 INFO models.py(2490) - Loading model from folder models/animefull-latest\n",
            "Loading model from models/animefull-latest/model.ckpt\n",
            "\u001b[90m2022-10-15T10:38:28Z\u001b[0m \u001b[32mINF\u001b[0m Connection 580a1f8e-027d-48fa-ad89-15c50ba662cf registered \u001b[36mconnIndex=\u001b[0m1 \u001b[36mip=\u001b[0m198.41.192.227 \u001b[36mlocation=\u001b[0mDFW\n",
            "\u001b[90m2022-10-15T10:38:29Z\u001b[0m \u001b[32mINF\u001b[0m Connection 6b23fb3f-be5a-4f56-9a7b-4ae6384f37b4 registered \u001b[36mconnIndex=\u001b[0m2 \u001b[36mip=\u001b[0m198.41.200.233 \u001b[36mlocation=\u001b[0mORD\n",
            "\u001b[90m2022-10-15T10:38:30Z\u001b[0m \u001b[32mINF\u001b[0m Connection 52595a1f-e4e5-49af-9898-bf3b8620fa83 registered \u001b[36mconnIndex=\u001b[0m3 \u001b[36mip=\u001b[0m198.41.192.47 \u001b[36mlocation=\u001b[0mDFW\n",
            "\u001b[90m2022-10-15T10:38:37Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:6969: connect: connection refused\"\u001b[0m \u001b[36mcfRay=\u001b[0m75a7ed1e9269f1e8-DFW \u001b[36moriginService=\u001b[0mhttp://localhost:6969\n",
            "\u001b[90m2022-10-15T10:38:37Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:6969: connect: connection refused\"\u001b[0m \u001b[36mconnIndex=\u001b[0m1 \u001b[36mdest=\u001b[0mhttps://creates-communist-jokes-bookstore.trycloudflare.com/ \u001b[36mip=\u001b[0m198.41.192.227 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2022-10-15T10:38:38Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:6969: connect: connection refused\"\u001b[0m \u001b[36mcfRay=\u001b[0m75a7ed21e3acf1e8-DFW \u001b[36moriginService=\u001b[0mhttp://localhost:6969\n",
            "\u001b[90m2022-10-15T10:38:38Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:6969: connect: connection refused\"\u001b[0m \u001b[36mconnIndex=\u001b[0m1 \u001b[36mdest=\u001b[0mhttps://creates-communist-jokes-bookstore.trycloudflare.com/favicon.ico \u001b[36mip=\u001b[0m198.41.192.227 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[32mINF\u001b[0m Unregistered tunnel connection \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve quic connection \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.193\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[32mINF\u001b[0m Unregistered tunnel connection \u001b[36mconnIndex=\u001b[0m2\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve quic connection \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m2 \u001b[36mip=\u001b[0m198.41.200.233\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[32mINF\u001b[0m Unregistered tunnel connection \u001b[36mconnIndex=\u001b[0m3\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve quic connection \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m3 \u001b[36mip=\u001b[0m198.41.192.47\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[32mINF\u001b[0m Unregistered tunnel connection \u001b[36mconnIndex=\u001b[0m1\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve quic connection \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m1 \u001b[36mip=\u001b[0m198.41.192.227\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2022-10-15T10:39:04Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "Keeping EMAs of 688.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Started frozen clip class\n",
            "2022-10-15 10:39:35,664 INFO models.py(2490) - Using VAE from models/animevae.pt\n",
            "2022-10-15 10:39:35,665 INFO models.py(2490) - CLIP: Using penultimate layer\n",
            "2022-10-15 10:39:35,665 INFO models.py(2490) - Using EMA\n",
            "2022-10-15 10:40:08,351 INFO config.py(2490) - Models loaded in 99.58s\n",
            "Loaded tag suggestion model using phrase embeddings\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2490\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "2022-10-15 10:40:10,416 INFO main.py(2490) - FastAPI Started, serving\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:6969\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "2022-10-15 10:40:13,126 INFO main.py(2490) - FastAPI Shutdown, exiting\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m2490\u001b[0m]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-54e8e10bccbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'env'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SAVE_FILES=1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./venv/bin/python -m uvicorn --host 0.0.0.0 --port=6969 main:app & bore local 6969 --to bore.pub & cloudflared tunnel --url localhost:6969 '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   result = _run_command(\n\u001b[0;32m--> 437\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    438\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}